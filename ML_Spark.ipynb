{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#vérification du bon fonctionnement et du lancement du sc\n",
    "#import des packages nécessaires\n",
    "#sc\n",
    "import utils\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pyspark.mllib.stat import Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1-nom Fichier</th>\n",
       "      <th>2-nb Col</th>\n",
       "      <th>3-nb Lignes</th>\n",
       "      <th>4-nb Lignes error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>glass</td>\n",
       "      <td>9</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  1-nom Fichier  2-nb Col  3-nb Lignes  4-nb Lignes error\n",
       "0         glass         9          214                  0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chargement du fichier glass.csv\n",
    "nomF = \"glass\"\n",
    "dataGlass = sc.textFile(\"file:/C:/spark-1.6.0-bin-hadoop2.4/\"+nomF+\".csv\")\n",
    "#séparation des colonnes\n",
    "lines = dataGlass.map(lambda line: utils.toRow(line, ';'))\n",
    "#ligne 1 = header -> save header\n",
    "nomColInit = lines.take(1)[0]\n",
    "#save nb col\n",
    "nbColInit = len(nomColInit)\n",
    "#sruppression du header\n",
    "parts = lines.filter(lambda line: nomColInit != line) \n",
    "parts = parts.filter(lambda line: len(line) == nbColInit)\n",
    "#cptage nb lignes (sans le header)   \n",
    "nbLignesInit = parts.count()\n",
    "#cptage erreur : nb col header != nb col de la ligne \n",
    "partsError = parts.filter(lambda line: len(line) != nbColInit)\n",
    "nbErrorL = partsError.count()\n",
    "#tableau résumé \n",
    "data = pd.DataFrame({'1-nom Fichier':[nomF],\n",
    "                       '2-nb Col':[nbColInit],\n",
    "                       '3-nb Lignes': [nbLignesInit],\n",
    "                        '4-nb Lignes error': [nbErrorL]})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAEKCAYAAADdKRa4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbYwxxqQwS9TGGGNMCvt/XJUS873+56sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc090240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#La variable à prédire est le type : col=9\n",
    "#nomColInit[9]\n",
    "\n",
    "##################################################\n",
    "# Répartition des classes de la variable à prédire\n",
    "\n",
    "# traitement sur la 9ème col\n",
    "typeV = parts.map(lambda line: line[9])\n",
    "# map + reduce\n",
    "distribType = typeV.map(lambda typeT: (typeT, 1)).reduceByKey(lambda a, b: a+b)\n",
    "i = 0\n",
    "name = []\n",
    "data = []\n",
    "while i <6:\n",
    "    name = name + [distribType.collect()[i][0]]\n",
    "    data = data +[int(distribType.collect()[i][1])]\n",
    "    i+=1\n",
    "plt.pie(data, labels=name, autopct='%1.1f%%', startangle=90, shadow=True)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sodium</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Aluminum</th>\n",
       "      <th>Silicon</th>\n",
       "      <th>Potassium</th>\n",
       "      <th>Calcium</th>\n",
       "      <th>Barium</th>\n",
       "      <th>Iron</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13.407850</td>\n",
       "      <td>2.684533</td>\n",
       "      <td>1.444907</td>\n",
       "      <td>72.650935</td>\n",
       "      <td>0.497056</td>\n",
       "      <td>8.956963</td>\n",
       "      <td>0.175047</td>\n",
       "      <td>0.057009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.816604</td>\n",
       "      <td>1.442408</td>\n",
       "      <td>0.499270</td>\n",
       "      <td>0.774546</td>\n",
       "      <td>0.652192</td>\n",
       "      <td>1.423153</td>\n",
       "      <td>0.497219</td>\n",
       "      <td>0.097439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10.730000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>69.810000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.430000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.907500</td>\n",
       "      <td>2.115000</td>\n",
       "      <td>1.190000</td>\n",
       "      <td>72.280000</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>8.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.300000</td>\n",
       "      <td>3.480000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>72.790000</td>\n",
       "      <td>0.555000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.825000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>1.630000</td>\n",
       "      <td>73.087500</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>9.172500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.380000</td>\n",
       "      <td>4.490000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>75.410000</td>\n",
       "      <td>6.210000</td>\n",
       "      <td>16.190000</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>0.510000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Sodium   Magnesium    Aluminum     Silicon   Potassium     Calcium  \\\n",
       "count  214.000000  214.000000  214.000000  214.000000  214.000000  214.000000   \n",
       "mean    13.407850    2.684533    1.444907   72.650935    0.497056    8.956963   \n",
       "std      0.816604    1.442408    0.499270    0.774546    0.652192    1.423153   \n",
       "min     10.730000    0.000000    0.290000   69.810000    0.000000    5.430000   \n",
       "25%     12.907500    2.115000    1.190000   72.280000    0.122500    8.240000   \n",
       "50%     13.300000    3.480000    1.360000   72.790000    0.555000    8.600000   \n",
       "75%     13.825000    3.600000    1.630000   73.087500    0.610000    9.172500   \n",
       "max     17.380000    4.490000    3.500000   75.410000    6.210000   16.190000   \n",
       "\n",
       "           Barium        Iron  \n",
       "count  214.000000  214.000000  \n",
       "mean     0.175047    0.057009  \n",
       "std      0.497219    0.097439  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    0.000000  \n",
       "50%      0.000000    0.000000  \n",
       "75%      0.000000    0.100000  \n",
       "max      3.150000    0.510000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### En trichant #####\n",
    "# Utilisation de pandas pour résumer les données + afficher la matrice de corrélation\n",
    "df = pd.read_csv(\"file:/C:/spark-1.6.0-bin-hadoop2.4/\"+nomF+\".csv\", sep = \";\",header=0)\n",
    "df.describe()\n",
    "# Matrice de corrélation\n",
    "# print(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.34078505e+01   2.68453271e+00   1.44490654e+00   7.26509346e+01\n",
      "   4.97056075e-01   8.95696262e+00   1.75046729e-01   5.70093458e-02]\n",
      "[ 0.66684137  2.08054039  0.24927018  0.59992119  0.4253542   2.02536585\n",
      "  0.24722699  0.0094943 ]\n",
      "[ 214.  172.  214.  214.  184.  214.   38.   70.]\n"
     ]
    }
   ],
   "source": [
    "# Utilisation du package mllib\n",
    "# Basics Statistics\n",
    "partsNum = parts.map(lambda line: line[0:8])\n",
    "summary = Statistics.colStats(partsNum)\n",
    "print(summary.mean())\n",
    "print(summary.variance())\n",
    "print(summary.numNonzeros())\n",
    "Statistics.corr(partsNum, method=\"pearson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1-Méthode</th>\n",
       "      <th>2-Précision</th>\n",
       "      <th>3-Rappel</th>\n",
       "      <th>4-F_mesure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>0.355919</td>\n",
       "      <td>0.368657</td>\n",
       "      <td>1.105971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    1-Méthode  2-Précision  3-Rappel  4-F_mesure\n",
       "0  NaiveBayes     0.355919  0.368657    1.105971"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############\n",
    "# Naive Bayes #\n",
    "###############\n",
    "\n",
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import utils_mesure\n",
    "import pandas as pd\n",
    "\n",
    "def parseLine(line):\n",
    "    parts = line.split(';')\n",
    "    label = float(parts[8])\n",
    "    features = Vectors.dense([float(x) for x in parts[0:7]])\n",
    "    return LabeledPoint(label, features)\n",
    "\n",
    "data = sc.textFile(\"file:/C:/spark-1.6.0-bin-hadoop2.4/glass_svm.csv\")\n",
    "\n",
    "# suppression du header\n",
    "nomColInit = data.first()\n",
    "data2 = data.filter(lambda line: nomColInit != line) \n",
    "data = data2.map(parseLine)\n",
    "\n",
    "# Echantillonnage 60% entrainement et 40% test\n",
    "training, test = data.randomSplit([0.6, 0.4], seed=0)\n",
    "# construction d'un modèle Bayesien sur l'échantillon d'entrainement\n",
    "model = NaiveBayes.train(training, 1.0)\n",
    "\n",
    "# Application sur l'echantillon test\n",
    "predictionAndLabel = test.map(lambda p: (model.predict(p.features), p.label))\n",
    "\n",
    "# Calcul des indicateurs du modèle\n",
    "accuracy = 1.0 * predictionAndLabel.filter(lambda (x, v): x == v ).count() / test.count()\n",
    "pG = 0\n",
    "rG = 0\n",
    "i = 1\n",
    "while i < 7:\n",
    "    a = utils_mesure.matriceConf(predictionAndLabel, i)\n",
    "    pG = pG + utils_mesure.precision(a)\n",
    "    rG = rG + utils_mesure.rappel(a)\n",
    "    i += 1\n",
    "pG = pG /6\n",
    "rG = rG /6\n",
    "fM = utils_mesure.f_mesure(rG, pG)\n",
    "\n",
    "# Résumée des mesures du modèle\n",
    "data = pd.DataFrame({'1-Méthode':[\"NaiveBayes\"],\n",
    "                    '2-Précision':[pG],\n",
    "                       '3-Rappel':[rG],\n",
    "                       '4-F_mesure': [fM]})\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### En cours #####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o55.trainSVMModelWithSGD.\n: org.apache.spark.SparkException: Input validation failed.\r\n\tat org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm.run(GeneralizedLinearAlgorithm.scala:251)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainRegressionModel(PythonMLLibAPI.scala:94)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainSVMModelWithSGD(PythonMLLibAPI.scala:233)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\r\n\tat py4j.Gateway.invoke(Gateway.java:259)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b17d802cdebc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Build the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVMWithSGD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsedData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# Evaluating the model on training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\pyspark\\mllib\\classification.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(cls, data, iterations, step, regParam, miniBatchFraction, initialWeights, regType, intercept, validateData, convergenceTol)\u001b[0m\n\u001b[0;32m    528\u001b[0m                                  bool(intercept), bool(validateData), float(convergenceTol))\n\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_regression_train_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSVMModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitialWeights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\pyspark\\mllib\\regression.pyc\u001b[0m in \u001b[0;36m_regression_train_wrapper\u001b[1;34m(train_func, modelClass, data, initial_weights)\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodelClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumFeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumClasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m         \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_convert_to_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodelClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\pyspark\\mllib\\classification.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(rdd, i)\u001b[0m\n\u001b[0;32m    526\u001b[0m             return callMLlibFunc(\"trainSVMModelWithSGD\", rdd, int(iterations), float(step),\n\u001b[0;32m    527\u001b[0m                                  \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregParam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminiBatchFraction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregType\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m                                  bool(intercept), bool(validateData), float(convergenceTol))\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_regression_train_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSVMModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitialWeights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\pyspark\\mllib\\common.pyc\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[1;34m(name, *args)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\pyspark\\mllib\\common.pyc\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\lib\\py4j-0.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\pyspark\\sql\\utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\lib\\py4j-0.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o55.trainSVMModelWithSGD.\n: org.apache.spark.SparkException: Input validation failed.\r\n\tat org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm.run(GeneralizedLinearAlgorithm.scala:251)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainRegressionModel(PythonMLLibAPI.scala:94)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainSVMModelWithSGD(PythonMLLibAPI.scala:233)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\r\n\tat py4j.Gateway.invoke(Gateway.java:259)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "# Classification et regression \n",
    "# Méthodes linéaires\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "nomF = \"glass_svm\"\n",
    "# Load and parse the data\n",
    "def parsePoint(line):\n",
    "    values = [x for x in line.split(';')]\n",
    "    return LabeledPoint(values[8], values[0:7])\n",
    "\n",
    "data = sc.textFile(\"file:/C:/spark-1.6.0-bin-hadoop2.4/\"+nomF+\".csv\")\n",
    "data.first()\n",
    "nomColInit = data.first()\n",
    "#sruppression du header\n",
    "data2 = data.filter(lambda line: nomColInit != line) \n",
    "\n",
    "parsedData = data2.map(parsePoint)\n",
    "\n",
    "# Build the model\n",
    "model = SVMWithSGD.train(parsedData, iterations=100)\n",
    "\n",
    "# Evaluating the model on training data\n",
    "labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(parsedData.count())\n",
    "print(\"Training Error = \" + str(trainErr))\n",
    "\n",
    "# Save and load model\n",
    "#model.save(sc, \"myModelPath\")\n",
    "#sameModel = SVMModel.load(sc, \"myModelPath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1353.trainLogisticRegressionModelWithLBFGS.\n: org.apache.spark.SparkException: Input validation failed.\r\n\tat org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm.run(GeneralizedLinearAlgorithm.scala:251)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainRegressionModel(PythonMLLibAPI.scala:94)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainLogisticRegressionModelWithLBFGS(PythonMLLibAPI.scala:293)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\r\n\tat py4j.Gateway.invoke(Gateway.java:259)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-e2a099c004a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegressionWithLBFGS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegressionWithLBFGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsedData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumClasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\pyspark\\mllib\\classification.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(cls, data, iterations, initialWeights, regParam, regType, intercept, corrections, tolerance, validateData, numClasses)\u001b[0m\n\u001b[0;32m    380\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m                     \u001b[0minitialWeights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnumClasses\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_regression_train_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitialWeights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\pyspark\\mllib\\regression.py\u001b[0m in \u001b[0;36m_regression_train_wrapper\u001b[1;34m(train_func, modelClass, data, initial_weights)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodelClass\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         weights, intercept, numFeatures, numClasses = train_func(\n\u001b[1;32m--> 211\u001b[1;33m             data, _convert_to_vector(initial_weights))\n\u001b[0m\u001b[0;32m    212\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodelClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumFeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumClasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\pyspark\\mllib\\classification.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(rdd, i)\u001b[0m\n\u001b[0;32m    370\u001b[0m             return callMLlibFunc(\"trainLogisticRegressionModelWithLBFGS\", rdd, int(iterations), i,\n\u001b[0;32m    371\u001b[0m                                  \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregParam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintercept\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrections\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 372\u001b[1;33m                                  float(tolerance), bool(validateData), int(numClasses))\n\u001b[0m\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minitialWeights\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\pyspark\\mllib\\common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[1;34m(name, *args)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\pyspark\\mllib\\common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\lib\\py4j-0.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\pyspark\\sql\\utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\lib\\py4j-0.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1353.trainLogisticRegressionModelWithLBFGS.\n: org.apache.spark.SparkException: Input validation failed.\r\n\tat org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm.run(GeneralizedLinearAlgorithm.scala:251)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainRegressionModel(PythonMLLibAPI.scala:94)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainLogisticRegressionModelWithLBFGS(PythonMLLibAPI.scala:293)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\r\n\tat py4j.Gateway.invoke(Gateway.java:259)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "\n",
    "model = LogisticRegressionWithLBFGS.train(parsedData, numClasses=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.366459627329\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o63.save.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/C:/spark-1.6.0-bin-hadoop2.4/myModelPath2/metadata already exists\r\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1179)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1156)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1443)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1422)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1422)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1422)\r\n\tat org.apache.spark.mllib.classification.impl.GLMClassificationModel$SaveLoadV1_0$.save(GLMClassificationModel.scala:61)\r\n\tat org.apache.spark.mllib.classification.LogisticRegressionModel.save(LogisticRegression.scala:159)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\r\n\tat py4j.Gateway.invoke(Gateway.java:259)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-206ffc579c9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Save and load model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"myModelPath2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0msameModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"myModelPath2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\pyspark\\mllib\\classification.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sc, path)\u001b[0m\n\u001b[0;32m    241\u001b[0m         java_model = sc._jvm.org.apache.spark.mllib.classification.LogisticRegressionModel(\n\u001b[0;32m    242\u001b[0m             _py2java(sc, self._coeff), self.intercept, self.numFeatures, self.numClasses)\n\u001b[1;32m--> 243\u001b[1;33m         \u001b[0mjava_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\lib\\py4j-0.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\pyspark\\sql\\utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\lib\\py4j-0.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o63.save.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/C:/spark-1.6.0-bin-hadoop2.4/myModelPath2/metadata already exists\r\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1179)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1156)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1443)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1422)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1422)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1422)\r\n\tat org.apache.spark.mllib.classification.impl.GLMClassificationModel$SaveLoadV1_0$.save(GLMClassificationModel.scala:61)\r\n\tat org.apache.spark.mllib.classification.LogisticRegressionModel.save(LogisticRegression.scala:159)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\r\n\tat py4j.Gateway.invoke(Gateway.java:259)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Load and parse the data\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])\n",
    "\n",
    "data = sc.textFile(\"data/mllib/sample_svm_data.txt\")\n",
    "parsedData = data.map(parsePoint)\n",
    "\n",
    "# Build the model\n",
    "model = LogisticRegressionWithLBFGS.train(parsedData)\n",
    "\n",
    "# Evaluating the model on training data\n",
    "labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(parsedData.count())\n",
    "print(\"Training Error = \" + str(trainErr))\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, \"myModelPath2\")\n",
    "sameModel = LogisticRegressionModel.load(sc, \"myModelPath2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o301.trainDecisionTreeModel.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 41.0 failed 1 times, most recent failure: Lost task 1.0 in stage 41.0 (TID 74, localhost): java.lang.IllegalArgumentException: EntropyAggregator given label 6.0 but requires label < numClasses (= 6).\r\n\tat org.apache.spark.mllib.tree.impurity.EntropyAggregator.update(Entropy.scala:98)\r\n\tat org.apache.spark.mllib.tree.impl.DTStatsAggregator.update(DTStatsAggregator.scala:100)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$.orderedBinSeqOp(DecisionTree.scala:424)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$.org$apache$spark$mllib$tree$DecisionTree$$nodeBinSeqOp$1(DecisionTree.scala:516)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$$anonfun$org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1$1.apply(DecisionTree.scala:541)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$$anonfun$org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1$1.apply(DecisionTree.scala:538)\r\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:109)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$.org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1(DecisionTree.scala:538)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$$anonfun$6$$anonfun$apply$8.apply(DecisionTree.scala:633)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$$anonfun$6$$anonfun$apply$8.apply(DecisionTree.scala:633)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$$anonfun$6.apply(DecisionTree.scala:633)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$$anonfun$6.apply(DecisionTree.scala:622)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\r\n\tat scala.Option.foreach(Option.scala:236)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:741)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:740)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:740)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$.findBestSplits(DecisionTree.scala:651)\r\n\tat org.apache.spark.mllib.tree.RandomForest.run(RandomForest.scala:233)\r\n\tat org.apache.spark.mllib.tree.DecisionTree.run(DecisionTree.scala:60)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$.train(DecisionTree.scala:86)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainDecisionTreeModel(PythonMLLibAPI.scala:748)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\r\n\tat py4j.Gateway.invoke(Gateway.java:259)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.IllegalArgumentException: EntropyAggregator given label 6.0 but requires label < numClasses (= 6).\r\n\tat org.apache.spark.mllib.tree.impurity.EntropyAggregator.update(Entropy.scala:98)\r\n\tat org.apache.spark.mllib.tree.impl.DTStatsAggregator.update(DTStatsAggregator.scala:100)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$.orderedBinSeqOp(DecisionTree.scala:424)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$.org$apache$spark$mllib$tree$DecisionTree$$nodeBinSeqOp$1(DecisionTree.scala:516)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$$anonfun$org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1$1.apply(DecisionTree.scala:541)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$$anonfun$org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1$1.apply(DecisionTree.scala:538)\r\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:109)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$.org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1(DecisionTree.scala:538)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$$anonfun$6$$anonfun$apply$8.apply(DecisionTree.scala:633)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$$anonfun$6$$anonfun$apply$8.apply(DecisionTree.scala:633)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$$anonfun$6.apply(DecisionTree.scala:633)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$$anonfun$6.apply(DecisionTree.scala:622)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4bc9726e4b13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMLUtils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m model2 = DecisionTree.trainClassifier(training, numClasses=6, categoricalFeaturesInfo={},\n\u001b[1;32m----> 4\u001b[1;33m                                      impurity='entropy', maxDepth=5, maxBins=32)\n\u001b[0m",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\pyspark\\mllib\\tree.pyc\u001b[0m in \u001b[0;36mtrainClassifier\u001b[1;34m(cls, data, numClasses, categoricalFeaturesInfo, impurity, maxDepth, maxBins, minInstancesPerNode, minInfoGain)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \"\"\"\n\u001b[0;32m    205\u001b[0m         return cls._train(data, \"classification\", numClasses, categoricalFeaturesInfo,\n\u001b[1;32m--> 206\u001b[1;33m                           impurity, maxDepth, maxBins, minInstancesPerNode, minInfoGain)\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\pyspark\\mllib\\tree.pyc\u001b[0m in \u001b[0;36m_train\u001b[1;34m(cls, data, type, numClasses, features, impurity, maxDepth, maxBins, minInstancesPerNode, minInfoGain)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLabeledPoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"the data should be RDD of LabeledPoint\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         model = callMLlibFunc(\"trainDecisionTreeModel\", data, type, numClasses, features,\n\u001b[1;32m--> 146\u001b[1;33m                               impurity, maxDepth, maxBins, minInstancesPerNode, minInfoGain)\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDecisionTreeModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\pyspark\\mllib\\common.pyc\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[1;34m(name, *args)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\pyspark\\mllib\\common.pyc\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\lib\\py4j-0.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\pyspark\\sql\\utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-1.6.0-bin-hadoop2.4\\python\\lib\\py4j-0.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o301.trainDecisionTreeModel.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 41.0 failed 1 times, most recent failure: Lost task 1.0 in stage 41.0 (TID 74, localhost): java.lang.IllegalArgumentException: EntropyAggregator given label 6.0 but requires label < numClasses (= 6).\r\n\tat org.apache.spark.mllib.tree.impurity.EntropyAggregator.update(Entropy.scala:98)\r\n\tat org.apache.spark.mllib.tree.impl.DTStatsAggregator.update(DTStatsAggregator.scala:100)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$.orderedBinSeqOp(DecisionTree.scala:424)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$.org$apache$spark$mllib$tree$DecisionTree$$nodeBinSeqOp$1(DecisionTree.scala:516)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$$anonfun$org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1$1.apply(DecisionTree.scala:541)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$$anonfun$org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1$1.apply(DecisionTree.scala:538)\r\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:109)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$.org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1(DecisionTree.scala:538)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$$anonfun$6$$anonfun$apply$8.apply(DecisionTree.scala:633)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$$anonfun$6$$anonfun$apply$8.apply(DecisionTree.scala:633)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$$anonfun$6.apply(DecisionTree.scala:633)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$$anonfun$6.apply(DecisionTree.scala:622)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\r\n\tat scala.Option.foreach(Option.scala:236)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:741)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:740)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:740)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$.findBestSplits(DecisionTree.scala:651)\r\n\tat org.apache.spark.mllib.tree.RandomForest.run(RandomForest.scala:233)\r\n\tat org.apache.spark.mllib.tree.DecisionTree.run(DecisionTree.scala:60)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$.train(DecisionTree.scala:86)\r\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.trainDecisionTreeModel(PythonMLLibAPI.scala:748)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\r\n\tat py4j.Gateway.invoke(Gateway.java:259)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.IllegalArgumentException: EntropyAggregator given label 6.0 but requires label < numClasses (= 6).\r\n\tat org.apache.spark.mllib.tree.impurity.EntropyAggregator.update(Entropy.scala:98)\r\n\tat org.apache.spark.mllib.tree.impl.DTStatsAggregator.update(DTStatsAggregator.scala:100)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$.orderedBinSeqOp(DecisionTree.scala:424)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$.org$apache$spark$mllib$tree$DecisionTree$$nodeBinSeqOp$1(DecisionTree.scala:516)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$$anonfun$org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1$1.apply(DecisionTree.scala:541)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$$anonfun$org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1$1.apply(DecisionTree.scala:538)\r\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:109)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$.org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1(DecisionTree.scala:538)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$$anonfun$6$$anonfun$apply$8.apply(DecisionTree.scala:633)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$$anonfun$6$$anonfun$apply$8.apply(DecisionTree.scala:633)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$$anonfun$6.apply(DecisionTree.scala:633)\r\n\tat org.apache.spark.mllib.tree.DecisionTree$$anonfun$6.apply(DecisionTree.scala:622)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "model2 = DecisionTree.trainClassifier(training, numClasses=6, categoricalFeaturesInfo={},\n",
    "                                     impurity='entropy', maxDepth=5, maxBins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0416666666667\n",
      "Learned classification tree model:\n",
      "DecisionTreeModel classifier of depth 2 with 5 nodes\n",
      "  If (feature 434 <= 0.0)\n",
      "   If (feature 99 <= 0.0)\n",
      "    Predict: 0.0\n",
      "   Else (feature 99 > 0.0)\n",
      "    Predict: 1.0\n",
      "  Else (feature 434 > 0.0)\n",
      "   Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Load and parse the data file into an RDD of LabeledPoint.\n",
    "data = MLUtils.loadLibSVMFile(sc, 'data/mllib/sample_libsvm_data.txt')\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "model = DecisionTree.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification tree model:')\n",
    "print(model.toDebugString())\n",
    "\n",
    "# Save and load model\n",
    "#model.save(sc, \"target/tmp/myDecisionTreeClassificationModel\")\n",
    "#sameModel = DecisionTreeModel.load(sc, \"target/tmp/myDecisionTreeClassificationModel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
